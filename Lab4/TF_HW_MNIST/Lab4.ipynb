{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Лабораторная работа 4\n",
    "\n",
    "**Распознавание символов**\n",
    "(в моем случае цифр)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаться будем на MNIST(Mixed National Institute of Standards and Technology), которая содержит более 70000 изображений рукописных цифр."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение сети\n",
    "\n",
    "После обучения сети, параметры мы запишем в model/model.ckpt, что бы далее использовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy 0.1\n",
      "step 1000, training accuracy 0.98\n",
      "step 2000, training accuracy 0.94\n",
      "step 3000, training accuracy 0.98\n",
      "step 4000, training accuracy 0.98\n",
      "step 5000, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 14000, training accuracy 0.98\n",
      "step 15000, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 19000, training accuracy 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import input_data\n",
    "import numpy as np\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "def change_size(X):\n",
    "    X1 = []\n",
    "    for image in X:\n",
    "        image = image.reshape((28, 28))\n",
    "        X1.append(image)\n",
    "    X = np.asarray(X1)\n",
    "    h = X.shape[1]\n",
    "    w = X.shape[2]\n",
    "\n",
    "    return X.reshape((X.shape[0], h, w, 1))\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def makeCNN(x, keep_prob):\n",
    "    # --- define CNN model\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    W_conv2 = weight_variable([3, 3, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    return y_conv\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "y_conv = makeCNN(x, keep_prob)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "saver = tf.train.Saver(keep_checkpoint_every_n_hours=1.0)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batchSize = 50\n",
    "\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    image = change_size(batch[0])\n",
    "    if i % 1000 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x: image, y: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: image, y: batch[1], keep_prob: 0.5})\n",
    "save_path = saver.save(sess, \"model/model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Распознование\n",
    "\n",
    "Из-за малой мощности моего ПК, обучимся по части выборки, если точно то по 3 000 картинкам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "7\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def makeCNN(x, keep_prob):\n",
    "    # --- define CNN model\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    W_conv2 = weight_variable([3, 3, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    return y_conv\n",
    "\n",
    "\n",
    "class handwritingDetection():\n",
    "    def __init__(self):\n",
    "        self.x = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"x\")\n",
    "        self.keep_prob = tf.placeholder(\"float\")\n",
    "        self.y_conv = makeCNN(self.x, self.keep_prob)\n",
    "\n",
    "        self._saver = tf.train.Saver()\n",
    "        self._session = tf.InteractiveSession()\n",
    "\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self._session.run(init_op)\n",
    "\n",
    "        self._saver.restore(self._session, \"model/model.ckpt\")\n",
    "\n",
    "    def detect(self, imgLocation):\n",
    "        cv_image = cv2.imread(imgLocation, 0)\n",
    "\n",
    "        ret, cv_image_binary = cv2.threshold(cv_image, 128, 255, cv2.THRESH_BINARY_INV)\n",
    "        cv_image_28 = cv2.resize(cv_image_binary, (28, 28))\n",
    "        np_image = np.reshape(cv_image_28, (1, 28, 28, 1))\n",
    "        predict_num = self._session.run(self.y_conv, feed_dict={self.x: np_image, self.keep_prob: 1.0})\n",
    "\n",
    "        return np.argmax(predict_num, 1)[0]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    detector = handwritingDetection()\n",
    "    print(detector.detect('image_7.png'))\n",
    "    print(detector.detect('image_3.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![7](image_7.png)\n",
    "![3](image_3.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
